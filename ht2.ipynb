{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 02, Inteligencia Artificial\n",
    "Autores:\n",
    "- Mark Albrand/ 21004\n",
    "- Jimena Hernández/ 21199\n",
    "- Melissa Pérez/ 21385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Preguntas Teóricas\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "#### 1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "Un MDP modela un problema de decisión sequencial en donde el sistema evoluciona en el tiempo y es controlado por un agente. La dinámica del sistema está determinada por una función de transición de probabilidad que mapea estados y acciones a otros estados.\n",
    "Los elementos de un MDP son:\n",
    "- Un conjunto finito de estados (S) : {s1,...,sn}, donde st denota el estado s ∈ S al tiempo t.\n",
    "- Un conjunto finito de acciones que pueden depender de cada estado, A(s), donde at ∈ A(s) denota la acción realizada en un estado s en el tiempo t.\n",
    "- Una función de recompensa (R(s,a,s')) que regresa un número real indicando lo deseado de estar en un estado s! ∈ S dado que en el estado s ∈ S se realizó la acción a ∈ A(s).\n",
    "- Una función de transición de estados dada como una distribución de probabilidad (P(s'|s,a)) que denota la probabilidad de llegar al estado s! ∈ S dado que se tomó la acción a ∈ A(s) en el estado s ∈ S, que también denotaremos como Φ(s, a, s!). (Sucar, 2018)\n",
    "\n",
    "#### 2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.\n",
    "- Política (π) hace referencia al conjunto de reglas o algoritmos de mapeo que el agente sigue para elegir acciones en cada estado.\n",
    "- La evaluación de políticas consiste en determinar la calidad de una política mediante la estimación del valor esperado de retorno a la misma. Tambíen, mide el desempeño en términos de su capacidad para lograr los objetivos.\n",
    "- La mejora de políticas implica modificar una política para hacerla mejor y se basa generalmente en la evaluación de la política.\n",
    "- La iteración de políticas es un proceso iterativo continuo que combina la evaluación con la mejora de políticas para encontrar la más óptima.\n",
    "\n",
    "En general, las políticas establecen las reglas para el desarrollo, la evaluación determina la efectividad, la mejora busca optimizarlas y la iteración asegura que el rendimiento óptimo será siempre ajustado. (Buczyński, 2023)\n",
    "\n",
    "#### 3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "El factor de descuento gamma controla la importancia relativa de las recompensas futuras en comparación con las recompensas inmediatas, se utiliza para modelar la preferencia del agente para obtener las recompensas. Este factor influye en la toma de decisiones al equilibrar las recompensas a corto y largo plazo. Un valor de gamma alto, favorece la toma de decisiones que maximizan la recompensa acumulada en el tiempo. Básicamente este factor es crucial para equilibrar las recompensas, planificar las acciones y evaluar las consecuencias futuras en la toma de decisiones. (Gite, 2017)\n",
    "\n",
    "#### 4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
    "Ambos algoritmos tienen un efoque para encontrar la mejor política. La diferencia es que el de iteración de valores calcula iterativamente los valores de los estados hasta que convergen a sus valores óptimos. Mientras que el de iteración de políticas, encuentra la política óptima directamente al mejorar iterativamente una política hasta que no se pueda mejorar más. (Tokuç, 2023)\n",
    "\n",
    "#### 5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.\n",
    "Algunos desafíos para MDP a gran escala serían \n",
    "- La complejidad computacional para resolverlo aumentando el tamaño del espacio de estados y acciones\n",
    "- La gran cantidad de memoria para almacenar la información \n",
    "- Explorar una gran cantidad de espacio de estados para encontrar la política óptima.\n",
    "\n",
    "Como enfoques potenciales para abordar los desafíos se podría \n",
    "- Utilizar algoritmos que aproximan el valor de los estados o la política\n",
    "- Paralelización para distribuir la computación en múltiples procesadores \n",
    "- Utilizar técnicas de muestreo para explorar el espacio de estados eficientemente.\n",
    "\n",
    "### Referencias:\n",
    "- Sucar, L. E. (2018). Procesos de decisión de Markov y aprendizaje por refuerzo. En Inteligencia Artificial (pp. 536). Alfaomega.\n",
    "- Buczyński, R. (2023). Understanding Markov Decision Processes. https://python.plainenglish.io/understanding-markov-decision-processes-17e852cd9981\n",
    "- Gite, S. (2017). Practical Reinforcement Learning. https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9\n",
    "- Tokuç. A. (2023). Value Iteration vs. Policy Iteration in Reinforcement Learning. https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration#:~:text=In%20policy%20iteration%2C%20we%20start%20with%20a%20fixed%20policy.,iteration%20algorithm%20updates%20the%20policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Preguntas Analíticas\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "#### 1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones para la toma de decisiones.\n",
    "\n",
    "#### 2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice estrategias para una toma de decisiones sólida en entornos inciertos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Preguntas Prácticas\n",
    "Desarrolle un agente básico capaz de resolver un problema simplificado del Proceso de Decisión de Markov (MDP).\n",
    "Considere utilizar un ejemplo bien conocido como el entorno 'Frozen Lake'. Proporcione el código Python para el proceso de toma de decisiones del agente basándose únicamente en los principios de los procesos de decisión de Markov. Recuerde que para este tipo de problema, el ambiente es una matriz de 4x4, y las acciones, pueden ser moverse hacia arriba, abajo, derecha, izquierda. Considere que el punto inicial siempre estará en la esquina opuesta del punto de meta. Es decir, puede tener hasta 4 configuraciones diferentes. Por ejemplo, el punto inicial puede estar en la coordenada (0, 0) y el punto de meta en la coordenada en la coordenada (3, 3). Además, la posición de los hoyos debe ser determinada aleatoriamente y no debe superar el ser más de 3. Es decir, si aleatoriamente se decide que sean 2 posiciones de hoyo, las coordenadas de estas deben ser determinadas de forma aleatoria.\n",
    "Asegúrese de usar “seed” para que sus resultados sean consistentes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
